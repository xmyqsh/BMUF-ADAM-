# BMUF-ADAM浅析

前天bing给推送了BMUF-ADAM算法，该算法可以让N台机器独立运行T步之后再Sync更新梯度，使通信频率降低T倍，
但这样会使NT的batchsize缩水成N，降低样本效率，该怎么把batchsize从N恢复到NT呢？
1. N台机器SGD可以看成N个随机过程，为了方便后面推导我们会假设随机过程是平稳的（在该场景下也确实是平稳的）。
这样后续的机器间的取平均、期望才有意义。（高大上）
2. BMUF使用T间冲量等效NT batchsize，而ADAM部分使用T内冲量恢复NT batchsize，
这里的不一致使本来严谨的数学逻辑出现了严重裂痕。


Anyway，但方法确实是work的，论文称该方法在OCR任务上可以逼近线性加速。

下面提出两点质疑：
1.
为什么选择小众的OCR而不选择大众的COCO数据集呢？
2.
因为所有的SGD方法都是基于一阶矩和二阶矩的，BMUF-ADAM理论上可轻松泛化到其它所以SGD方法，
这么简单的事情作者怎么没有做呢？

有分布式深度学习资源的朋友可以去验证以下，检验BMUF-ADAM的数学逻辑是否稳固。

深深感慨：
没什么比代码更严谨，包括数学。
